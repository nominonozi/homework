# 1.前言

###### 这个是边做变写的，很混乱0 0 

一开始看到这个数据归约也不知道是啥，马上丢到ai中。大概的理解就是，通过处理大量的数据，使得数据从某种

统计方式上的结果基本不变或者完全不变。

数据归约有很多种形式：降维，压缩，特征选择等

# 2.选择数据集

csdn，bilibili找了半天也没找不到合适的数据集（主要是还不知道这个作业要我做啥）（现在知道在哪里找了），本来想着做一个图片降低像素（应该也算是数据归约把）。后来还是问了ai，来学着做一下这个很经典的Iris数据集。

Iris数据集是有着150个样本，分为4个特征，3个类别的鸢尾科植物集

所以我们拥有150×（4+1）的数据。

# 3.过程（python）

刚开始了解到这个数据库可以直接在代码中导入，导入后是bunch对象，类似字典的数据结构

有如下的键

DESCR:数据集的说明

target_names：字符串数组；iris中是花的品种*'setosa' 'versicolor' 'virginica'*

feature_names：字符串列表；iris中是每个特征*'sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'*

type of data：数据；iris中是numpy数组的每个样品的4个特征数值

shape of data：数值的大致样子；iris显示为（150，4）表示有150个样本，4个数值

在数据归约前先检查数据的大体情况

```python
iris_local.plot(kind='box', subplots=True, layout=(2, 2), sharex=False, sharey=False)
plt.show()
iris_local.hist()
plt.show()
iris_local.plot()
plt.show()
```

用这些代码看这串数据的大概情况

然后用scikit-learn库提供的函数做特征选择

```python

selector = SelectKBest(score_func=f_classif, k=2)
X_new = selector.fit_transform(X, y)
selected_features = X.columns[selector.get_support()]
print("Selected Features:", selected_features)
```

 如此可以将4个数值删减到2个，方便后续的其他计算

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2).fit(X)
print("保留方差比例:", pca.explained_variance_ratio_.sum())

from sklearn.decomposition import PCA
pca = PCA(n_components=4).fit(X)
print("保留方差比例:", pca.explained_variance_ratio_.sum())
```

用这两串代码可以看出准确度只差了0.023左右

这里用到了pca，也就是主成分分析，它是一种常用的降维技术，目标是通过线性变换将数据投影到低维度空间中，同时尽可能保留原始数据中的方差信息。简单来说就是一个算法能够看出来降维前后的准确度损失了多少

下面还放了一个svc的但是我看不懂svc

![1744217721721](C:\Users\Liang\AppData\Roaming\Typora\typora-user-images\1744217721721.png)

[nominonozi/homework: 作业](https://github.com/nominonozi/homework)

LDA线性判别分析可以最大化类间差异同时最小化类内差异，用这个来做一下可视化

做LDA的步骤是

1.计算每个类别的均值向量

2.计算类内和类间的散布矩阵

3.计算广义特征值和特征向量

4.降序排列特征值，保留前k个特征向量

5.使用前几个特征向量将数据投影到新空间

首先先计算每个类别的均值向量

```python
mean_vectors = []
for cls in [0, 1, 2]:
    class_mean_vector = np.mean(iris_X[iris_Y==cls],axis=0)
    mean_vectors.append(class_mean_vector)
```

计算123三列的数据 np.mean是取平均值

计算散布矩阵（协方差矩阵之和）

```python
S_W = np.zeros((4,4))
for cls, mv in zip(range(3), mean_vectors):
    S_cls = np.zeros((4,4))
    for row in iris_X[iris_Y == cls]:
        row, mv = row.reshape(4,1), mv.reshape(4,1)
        S_cls += (row-mv).dot((row-mv).T)###
    S_W += S_cls
print(S_W)

all_mean = np.mean(iris_X, axis=0).reshape(4,1)
S_B = np.zeros((4,4))
for cls, mv in zip(range(3), mean_vectors):
    mv = mv.reshape(4,1)
    S_B += len(iris_X[iris_Y == cls])*(mv-all_mean).dot((mv-all_mean).T)###
print(S_B)
```

zero用于创造4x4的矩阵 mv代表四维 reshape用于将向量转换为列向量，将答案累加就是类内散度矩阵和类间散度矩阵两个是分别的计算公式

接下来计算广义特征值和特征向量

```
eig_vals, eig_vecs = np.linalg.eig(np.dot(np.linalg.inv(S_W), S_B))
eig_vals = eig_vals.real
eig_vecs = eig_vecs.real
for i in range(len(eig_vals)):
    eig_vec_cls = eig_vecs[:,i]
    print('特征向量 {}: {}'.format(i+1, eig_vec_cls))
    print('特征值 {}: {}'.format(i+1, eig_vals[i]))
```

Sw^−1×Sbw=λw

np.linalg.inv取倒数np.dot相乘np.linalg.eig分割数组和矩阵

降序排列特征值，保留前k个特征向量

```
linear_discriminants = eig_vecs.T[:2]
```

为了使数值更加精炼，选择最大的两个特征值的特征向量

使用前几个特征向量将数据投影到新空间（画图）

```
def plot(X, y, title, x_label, y_label):
  ax = plt.subplot(111)
  for label,marker,color in zip(range(3),('^', 's', 'o'),('blue', 'red', 'green')):
    plt.scatter(x=X[:,0].real[y == label], y=X[:,1].real[y == label], color=color, alpha=0.5, label=label_dict[label] )
    plt.xlabel(x_label)
    plt.ylabel(y_label)
  leg = plt.legend(loc='upper right', fancybox=True)
  leg.get_frame().set_alpha(0.5)
  plt.title(title)

lda_iris_projection = np.dot(iris_X, linear_discriminants.T)
lda_iris_projection[:5,]
```



![1745066957368](C:\Users\Liang\AppData\Roaming\Typora\typora-user-images\1745066957368.png)